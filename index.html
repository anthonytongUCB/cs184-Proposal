<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    <style>
        div.padded {
            padding-top: 0px;
            padding-right: 100px;
            padding-bottom: 0.25in;
            padding-left: 100px;
        }
    </style>
    <title>CS 184 Proposal</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Project Proposal: Adapting Human Motion to Character Mesh Movement</h1>
<h2 align="middle">David Deng, Chung Min Kim, Jerry Zhu, and Anthony Tong</h2>

<div class="padded">

    <p>
        Summary: Our project will allow a person to film a video of themselves and map their behavior to an animation with a character rig of a choice, by combining multiple existing frameworks as well as adding our own contributions by making sure that they come together smoothly by having a reach goal of using physics engines to optimize keypoint errors.
    </p>

    <h2 align="middle">Problem Description</h2>

    <p>
        Have you ever wanted to make a quick animation out of a character (e.g., Shrek) but lacked the animation experience to do so? Even if you had the experience, it would take a significant amount of time for an amateur to make sure that the character rig moved as exactly as you hoped. This project hopes to make animation creation more approachable to everyone.
        The goal of our project is to transfer human motion onto a skinned, rigged animated character. To do this, we will extract a sequence of 3D skeletons from a video using an out of the box deep human pose estimator. Next, we will refit the motion to the skeleton of the character using [5]. Lastly, we will rig the animated character using the extracted motion sequence with the MLDeform library [1].
    </p>

    <h2 align="middle">Goals and Deliverables</h2>

    <p>
        By the end of the project we plan to have a fully automated system that takes an input video of a human performing some action and produces a video of the animated character performing the same action. Ideally, we’d like to record one of ourselves holding an onion and saying “ogres are like onions,” and then transferring this action to Shrek. We hope to answer the question of whether or not this approach to motion retargeting is robust and produces visually appealing results.
        If time permits, we’d also like to investigate generating a more realistic 3D pose generation by simulating a humanoid in a differentiable physics engine to match the keypoints generated by out of the box human pose estimators. This will constrain the motion of the character to be more physically plausible. We would then pass this refined motion into the skeleton motion refitting stage.
    </p>

    <h2 align="middle">Schedule</h2>

    <ul>
        <li>
            Week 1:
            Confirm the frameworks required for our project and ensure that they are all compatible with one another.
        </li>
        <li>
            Week 2:
            Use the VIBE github to map input video of human movement to a 3D Skeleton
            Create a 3D Skeleton mesh and reproduce results to the paper “Fast and Deep Deformation Approximations” to create animated characters from 3D skeletons.
            Use [8] to optimize 3D keypoint error over shape, camera, ground plane, initial state, and per frame forces of a humanoid.
        </li>
        <li>
            Week 3:
            Adjust the shape of the 3D skeleton of human movement to the shape of Animated character using the paper in resource 5 or similar.
        </li>
        <li>
            Week 4:
            Create and finalize the entire pipeline for inputted video of human motion to generate adjustable 3D skeleton and finally output similar movement for Character animation.
        </li>
    </ul>

    <h2 align="middle">Resources</h2>
    <li><a href="http://graphics.berkeley.edu/papers/Bailey-FDD-2018-08/Bailey-FDD-2018-08.pdf">Fast and Deep Deformation Approximations</a></li>
    <li><a href="https://github.com/dgovil/MLDeform">https://github.com/keenon/diffdart</a></li>
    <li><a href="http://obrien.berkeley.edu/Prof._James_F._OBrien/Home.html">Prof. James O'Brien</a></li>
    <li><a href="https://stephanosterburg.gitbook.io/scrapbook/3deeplearning/fast-and-deep-deformation-approximations">Fast and Deep Deformation Approximations</a></li>
    <li><a href="https://arxiv.org/pdf/2005.05732.pdf">Skeleton-Aware Networks for Deep Motion Retargeting</a></li>
    <li><a href="https://github.com/DeepMotionEditing/deep-motion-editing">Deep-motion-editing</a></li>
    <li><a href="https://github.com/mkocabas/VIBE">VIBE: Video Inference for Human Body Pose and Shape Estimation</a></li>
    <li><a href="https://github.com/keenon/diffdart">Stanford DiffDART</a></li>





    </o>
</div>
</body>
</html>




